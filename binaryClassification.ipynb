{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Indicate the imported packages/libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the dataset and print the data information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a data frame\n",
    "\n",
    "df = pd.read_csv('dataset_assignment1.csv')\n",
    "\n",
    "# Print dataset information\n",
    "\n",
    "print(df.info()) # checking if there is any null in the dataset\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Understand the dataset\n",
    "Print out the number of samples for each class in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of samples for each class\n",
    "count_class = df['class'].value_counts()\n",
    "\n",
    "# Printing the number of samples for each class\n",
    "print('Number of samples for each class:')\n",
    "print(count_class)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some figures to visualize the dataset (e.g., histogram, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "\n",
    "# First create a grid of subplots\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
    "\n",
    "# Iterate over each feature\n",
    "for i, ax in enumerate(axs.flat, start=1):\n",
    "    feature_name = f'feature{i}'\n",
    "    for cls in df['class'].unique():\n",
    "        df[df['class'] == cls][feature_name].hist(bins=20, alpha=0.5, ax=ax, label=f'Class {cls}')\n",
    "    ax.set(title=f'{feature_name}', xlabel=feature_name, ylabel='frequency')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "# Calculate\n",
    "corr = df.corr()\n",
    "\n",
    "# Use seaborn to create a heatmap\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each class, print out the statistical description of features (e.g., the input variable x), such as mean, std, max and min values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical description of features\n",
    "\n",
    "# Group the data by class\n",
    "grouped_by_class = df.groupby('class')\n",
    "\n",
    "# Iterate over the feature columns\n",
    "for i in range(1, 10):\n",
    "    feature_name = f'feature{i}'\n",
    "    print(f'Statistical description of {feature_name} by class:')\n",
    "    print(grouped_by_class[feature_name].describe())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split data into a training dataset and a testing dataset (i.e., 80% v.s. 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into X features and y class labels\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Splitting the data into training data and testing data, with 80:20 respectively.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. For each classification algorithm you chose, please complete the below steps in Python:\n",
    "\n",
    "Train the model using the training dataset.\n",
    "    1. If there are hyperparameters in the algorithm, please use K-Fold Cross Validation (e.g., you could choose k = 5 for K-Fold Cross Validation) to tune the hyperparameters of the algorithm (e.g., explore the best value for hyperparameter “k” for KNN, or the best kernel for kernel SVM, etc.).\n",
    "    2. Please use different evaluation metrics, including precision, recall, accuracy, and F1-Score, to pick up a model that gives you the best result on the validation dataset (e.g., via the Cross Validation, for kNN model, which k value gives the best precision, recall, accuracy, and F1-Score respectively)\n",
    "\n",
    "Test the model (the best one you obtained from the above stage) on the testing dataset\n",
    "    1. Plot the confusion matrix\n",
    "    2. Please use different evaluation metrics, including precision, recall, accuracy, and F1-Score, to report the performance of the algorithm, you can use tables or plot figures to summarize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classification Algorithm\n",
    "\n",
    "# Range of k values\n",
    "k_values = list(range(1, 31,2))\n",
    "\n",
    "# Initialize lists to store the cross-val scores\n",
    "cv_scores_acc = []\n",
    "cv_scores_recall = []\n",
    "cv_scores_precision = []\n",
    "cv_scores_f1 = []\n",
    "\n",
    "# Perform K-Fold Cross Validation on range of k on each evaluation metric with k=5\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    # Accuracy \n",
    "    scores_acc = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores_acc.append(scores_acc.mean())\n",
    "\n",
    "    # Recall \n",
    "    scores_recall = cross_val_score(knn, X_train, y_train, cv=5, scoring='recall')\n",
    "    cv_scores_recall.append(scores_recall.mean())\n",
    "\n",
    "    # Precision \n",
    "    scores_precision = cross_val_score(knn, X_train, y_train, cv=5, scoring='precision')\n",
    "    cv_scores_precision.append(scores_precision.mean())\n",
    "\n",
    "    # F1 \n",
    "    scores_f1 = cross_val_score(knn, X_train, y_train, cv=5, scoring='f1')\n",
    "    cv_scores_f1.append(scores_f1.mean())\n",
    "    \n",
    "# Calculate best k value based on each evaluation metric\n",
    "best_k_acc = k_values[cv_scores_acc.index(max(cv_scores_acc))]\n",
    "best_k_recall = k_values[cv_scores_recall.index(max(cv_scores_recall))]\n",
    "best_k_precision = k_values[cv_scores_precision.index(max(cv_scores_precision))]\n",
    "best_k_f1 = k_values[cv_scores_f1.index(max(cv_scores_f1))]\n",
    "\n",
    "# Print the best k values based on each evaluation metric\n",
    "print(\"Best k value based on accuracy:\", best_k_acc)\n",
    "print(\"Best k value based on recall:\", best_k_recall)\n",
    "print(\"Best k value based on precision:\", best_k_precision)\n",
    "print(\"Best k value based on F1-score:\", best_k_f1)\n",
    "\n",
    "# Plot the cross-validation results for range of k values\n",
    "plt.plot(k_values, cv_scores_acc, label='Accuracy')\n",
    "plt.plot(k_values, cv_scores_recall, label='Recall')\n",
    "plt.plot(k_values, cv_scores_precision, label='Precision')\n",
    "plt.plot(k_values, cv_scores_f1, label='F1-score')\n",
    "\n",
    "# Plot the best k values against each evaluation metric\n",
    "plt.axvline(x=best_k_acc, linestyle='--', color='r', label='Best k (accuracy)')\n",
    "plt.axvline(x=best_k_recall, linestyle='--', color='g', label='Best k (recall)')\n",
    "plt.axvline(x=best_k_precision, linestyle='--', color='b', label='Best k (precision)')\n",
    "plt.axvline(x=best_k_f1, linestyle='--', color='y', label='Best k (F1-score)')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Score')\n",
    "plt.title('KNN Evaluation Metrics')\n",
    "plt.legend()\n",
    "\n",
    "# Plot \n",
    "plt.show()\n",
    "\n",
    "# Train the KNN model with the most optimal k value found using the training set\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN testing\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Use the testing data to calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "disp.ax_.set_title('Confusion Matrix for KNN Testing')\n",
    "plt.show()\n",
    "\n",
    "# Evaluation metrics\n",
    "\n",
    "# Calculate evaluation metrics using the best k value on testing data\n",
    "precision_knn = precision_score(y_test, y_pred_knn)\n",
    "recall_knn = recall_score(y_test, y_pred_knn)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "f1_knn = f1_score(y_test, y_pred_knn)\n",
    "\n",
    "# Print evaluation metrics using the best k value on testing data\n",
    "print(\"Precision based on a best k = 9:\", precision_knn)\n",
    "print(\"Recall based on a best k = 9:\", recall_knn)\n",
    "print(\"Accuracy based on a best k = 9:\", accuracy_knn)\n",
    "print(\"F1-Score Precision based on a best k = 9:\", f1_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classification Algorithm\n",
    "\n",
    "# Range of C values\n",
    "c_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Initialize lists to store the cross-val scores\n",
    "cv_scores_acc = []\n",
    "cv_scores_recall = []\n",
    "cv_scores_precision = []\n",
    "cv_scores_f1 = []\n",
    "\n",
    "# Perform K-Fold Cross Validation on range of C on each evaluation metric with k=5\n",
    "for c in c_values:\n",
    "    lr = LogisticRegression(C=c)\n",
    "\n",
    "    # Accuracy \n",
    "    scores_acc = cross_val_score(lr, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores_acc.append(scores_acc.mean())\n",
    "\n",
    "    # Recall \n",
    "    scores_recall = cross_val_score(lr, X_train, y_train, cv=5, scoring='recall')\n",
    "    cv_scores_recall.append(scores_recall.mean())\n",
    "\n",
    "    # Precision \n",
    "    scores_precision = cross_val_score(lr, X_train, y_train, cv=5, scoring='precision')\n",
    "    cv_scores_precision.append(scores_precision.mean())\n",
    "\n",
    "    # F1 \n",
    "    scores_f1 = cross_val_score(lr, X_train, y_train, cv=5, scoring='f1')\n",
    "    cv_scores_f1.append(scores_f1.mean())\n",
    "\n",
    "# Calculate the best C value based on each evaluation metric\n",
    "best_c_acc = c_values[cv_scores_acc.index(max(cv_scores_acc))]\n",
    "best_c_recall = c_values[cv_scores_recall.index(max(cv_scores_recall))]\n",
    "best_c_precision = c_values[cv_scores_precision.index(max(cv_scores_precision))]\n",
    "best_c_f1 = c_values[cv_scores_f1.index(max(cv_scores_f1))]\n",
    "\n",
    "# Print the best C values based on each evaluation metric\n",
    "print(\"Best C value based on accuracy:\", best_c_acc)\n",
    "print(\"Best C value based on recall:\", best_c_recall)\n",
    "print(\"Best C value based on precision:\", best_c_precision)\n",
    "print(\"Best C value based on F1-score:\", best_c_f1)\n",
    "\n",
    "# Plot the cross-validation results for range of C\n",
    "plt.plot(c_values, cv_scores_acc, label='Accuracy')\n",
    "plt.plot(c_values, cv_scores_recall, label='Recall')\n",
    "plt.plot(c_values, cv_scores_precision, label='Precision')\n",
    "plt.plot(c_values, cv_scores_f1, label='F1-score')\n",
    "\n",
    "# Plot the best C values against each evaluation metric\n",
    "plt.axvline(x=best_c_acc, linestyle='--', color='r', label='Best C (accuracy)')\n",
    "plt.axvline(x=best_c_recall, linestyle='--', color='g', label='Best C (recall)')\n",
    "plt.axvline(x=best_c_precision, linestyle='--', color='b', label='Best C (precision)')\n",
    "plt.axvline(x=best_c_f1, linestyle='--', color='y', label='Best C (F1-score)')\n",
    "plt.xlabel('Hyperparameter (C)')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Logistic Regression Evaluation Metrics')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "# Plot\n",
    "plt.show()\n",
    "\n",
    "# Train the logistic regression model with the best C value using the training set\n",
    "lr = LogisticRegression(C=0.1)\n",
    "lr.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logisitic Regression Testing\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Use the testing data to calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "disp.ax_.set_title('Confusion Matrix for Logistic Regression Testing')\n",
    "plt.show()\n",
    "\n",
    "# Evaluation metrics\n",
    "\n",
    "# Calculate evaluation metrics using the best C value on testing data\n",
    "precision_lr = precision_score(y_test, y_pred_lr)\n",
    "recall_lr = recall_score(y_test, y_pred_lr)\n",
    "f1_lr = f1_score(y_test, y_pred_lr)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "# Print evaluation metrics using the best C value on testing data\n",
    "print(\"Precision based on a best C = 0.1: \", precision_lr)\n",
    "print(\"Recall based on a best C = 0.1: \", recall_lr)\n",
    "print(\"F1-Score based on a best C = 0.1: \", f1_lr)\n",
    "print(\"Accuracy based on a best C = 0.1: \", accuracy_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classification Algorithm\n",
    "\n",
    "# Range of max_depth values\n",
    "max_depth_values = list(range(1, 21))\n",
    "\n",
    "# Initialize lists to store the cross-val scores\n",
    "cv_scores_acc = []\n",
    "cv_scores_recall = []\n",
    "cv_scores_precision = []\n",
    "cv_scores_f1 = []\n",
    "\n",
    "# Perform K-Fold Cross Validation on range of max_depth on each evaluation metric with k=5\n",
    "for max_depth in max_depth_values:\n",
    "    dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    \n",
    "    # Accuracy \n",
    "    scores_acc = cross_val_score(dt, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores_acc.append(scores_acc.mean())\n",
    "\n",
    "    # Recall \n",
    "    scores_recall = cross_val_score(dt, X_train, y_train, cv=5, scoring='recall')\n",
    "    cv_scores_recall.append(scores_recall.mean())\n",
    "\n",
    "    # Precision \n",
    "    scores_precision = cross_val_score(dt, X_train, y_train, cv=5, scoring='precision')\n",
    "    cv_scores_precision.append(scores_precision.mean())\n",
    "\n",
    "    # F1 \n",
    "    scores_f1 = cross_val_score(dt, X_train, y_train, cv=5, scoring='f1')\n",
    "    cv_scores_f1.append(scores_f1.mean())\n",
    "\n",
    "# Calculate best max depth value based on each evaluation metric\n",
    "best_depth_acc = max_depth_values[cv_scores_acc.index(max(cv_scores_acc))]\n",
    "best_depth_recall = max_depth_values[cv_scores_recall.index(max(cv_scores_recall))]\n",
    "best_depth_precision = max_depth_values[cv_scores_precision.index(max(cv_scores_precision))]\n",
    "best_depth_f1 = max_depth_values[cv_scores_f1.index(max(cv_scores_f1))]\n",
    "\n",
    "# Print the best max_depth values based on each evaluation metric\n",
    "print(\"Best max depth value based on accuracy:\", best_depth_acc)\n",
    "print(\"Best max depth value based on recall:\", best_depth_recall)\n",
    "print(\"Best max depth value based on precision:\", best_depth_precision)\n",
    "print(\"Best max depth value based on F1-score:\", best_depth_f1)\n",
    "\n",
    "# Plot the cross-validation results for range of max_depth\n",
    "plt.plot(max_depth_values, cv_scores_acc, label='Accuracy')\n",
    "plt.plot(max_depth_values, cv_scores_recall, label='Recall')\n",
    "plt.plot(max_depth_values, cv_scores_precision, label='Precision')\n",
    "plt.plot(max_depth_values, cv_scores_f1, label='F1-score')\n",
    "\n",
    "# Plot the best max depth values against each evaluation metric\n",
    "plt.axvline(x=best_depth_acc, linestyle='--', color='r', label='Best depth (accuracy)')\n",
    "plt.axvline(x=best_depth_recall, linestyle='--', color='g', label='Best depth (recall)')\n",
    "plt.axvline(x=best_depth_precision, linestyle='--', color='b', label='Best depth (precision)')\n",
    "plt.axvline(x=best_depth_f1, linestyle='--', color='y', label='Best depth (F1-score)')\n",
    "plt.xlabel('Maximum Depth')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Decision Tree Evaluation Metrics')\n",
    "plt.legend()\n",
    "\n",
    "# Plot\n",
    "plt.show()\n",
    "\n",
    "# Train the model with the selected hyperparameter on the training data\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Testing\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "\n",
    "# Use the testing data to calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "# Display  confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "disp.ax_.set_title('Confusion Matrix for Decision Tree Testing')\n",
    "plt.show()\n",
    "\n",
    "# Evaluation metrics\n",
    "\n",
    "# Calculate evaluation metrics using best max depth on testing data\n",
    "precision_dt = precision_score(y_test, y_pred_dt)\n",
    "recall_dt = recall_score(y_test, y_pred_dt)\n",
    "f1_dt = f1_score(y_test, y_pred_dt)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Print the evaluation metrics for max depth = 3\n",
    "print(\"Precision based on max depth = 3: \", precision_dt)\n",
    "print(\"Recall based on max depth = 3: \", recall_dt)\n",
    "print(\"F1-Score based on max depth = 3:\", f1_dt)\n",
    "print(\"Accuracy based on max depth = 3: \", accuracy_dt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
